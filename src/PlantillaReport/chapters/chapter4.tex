\documentclass[../main]{subfiles}
\usepackage{lastpage,xr,refcount,etoolbox}
%\externaldocument{../Appendices/Appendix1-CodigosBase}
\begin{document}


\chapter{``Unnary-Ssum Problem” en P}

{
\hypersetup{linkcolor=black}
\minitoc
\vspace{5mm}
}

Vamos a mostrar una implementación del algoritmo que resuelve el problema \textbf{Subset-Sum} usando programación dinámica. Mostraremos por qué la complejidad de la solución no pertenece a la clase \textbf{P} y por qué la versión del algoritmo para el problema con la entrada en unario, sí que pertenece a la clase\textbf{ P}. 
\section{Estudio del ``Subset-Sum Problem''}
Como ya he mencionado, el algoritmo que se ha implementado usa programación dinámica, por lo que lo primero que hay que hacer es definir su \textbf{función de Bellman}:
\begin{equation*}
    \text{dp}[i][j] = \begin{cases}
  \ true \ \ & \text{si } \  \ i\geq 0  \ \land \ j = 0 \\
  \ false & \text{si } \ \ i = 0 \ \land \ j > 0\\
  \ \text{dp}[i-1][j]  \lor  \text{dp}[i-1][j-S(i-1)] & \text{si } \ \ i,j>0 \ \land \ S(i-1) \leq j  \\
  \ \text{dp}[i-1][j] & \text{si } \ \ i,j>0 \ \land \ S(i-1) > j
\end{cases}
\end{equation*}
\begin{itemize}
    \item[\textbullet] \textbf{i, j} representan los índices en la tabla $\text{\textbf{dp}}$.
    \item[\textbullet] \textbf{S} es el conjunto de números enteros dados.
    \item[\textbullet] \textbf{\text{dp}[i][j]} es una función que devuelve si es posible sumar \textbf{j} utilizando los primeros \textbf{i} elementos del conjunto \textbf{S}.
\end{itemize}
\subsection{Implementación del algoritmo}
Con la \textbf{función de Bellman} ya construída, podemos pasar a una pseudo-implementación en \textbf{Java} del algoritmo que decide sobre nuestro problema:
\begin{javacode}
public class SubsetSum {
    // Conjunto de enteros
    private static final int[] S = {1, 2, 4, 2, 7, 5, 9, 4, 10, 3, 4};

    public static void main(String[] args) {
        Integer i = S.length;
        Integer j = 15;
        Boolean sol = dp(i, j);
        System.out.println(sol);
    }

    public static Boolean dp(Integer i, Integer j) {
        if(i >= 0 && j == 0) return true;
        if(i == 0 && j > 0) return false;
        if(i > 0 && j > 0 && S[i-1] <= j) {
            return dp(i-1, j) || dp(i-1, j-S[i-1]);
        }
        return dp(i-1, j);
    }
}
//Fin del algoritmo
\end{javacode}
Esta implementación del algoritmo utiliza un enfoque recursivo. Eso quiere decir que no crea una tabla sobre la que luego iterar para encontrar nuestras soluciones. Sin embargo, sí que está usando la pila de llamadas recursivas para guardar todos los cálculos intermedios. Esto quiere decir, que en lo referente al cómputo de la complejidad, ambos enfoques son equivalentes.

\subsection{Complejidad de la solución}
Volviendo a la idea original de que lo que realmente estamos creando es una tabla de \textbf{i} filas (i números en el conjunto) por \textbf{j} columnas (valor objetivo), la complejidad del algoritmo la representa una función \textbf{f(i, j)} que pertenece a \textbf{O(i*j)}. En una primera instancia concluiríamos con que el problema pertenece a la clase \textbf{P} porque hemos encontrado un algoritmo que decide sobre el problema en tiempo polinómico. No obstante, esta es en realidad una solución \textbf{pseudo-polínomica}.
\\\\
Lo que realmente ocurre es que no hemos tenido en cuenta que la codificación binaria de la entrada. La complejidad es exponencial cuando se mide en términos del tamaño de bits. Esto significa que el tiempo de ejecución incrementa exponencialmente con el tamaño de la entrada. Esto se debe a que la cantidad de posibles combinaciones de \textbf{n} bits es exponencial y el algoritmo debe considerar cada una de estas combinaciones, lo que lleva a un aumento exponencial en el tiempo de ejecución:
\begin{equation*}
    tam(i) \in O(2^i)
\end{equation*}
Luego la complejidad de nuestro algoritmo ya no estaría en tiempo polinómico para la entrada, sino en exponencial:
\begin{equation*}
    f(i, j) \in O(tam(i)*j) \Leftrightarrow  f(i, j) \in O(2^i*j)
\end{equation*}
\section{Estudio del ``Unary-Ssum Problem''}
Tanto el algoritmo como el estudio de la complejidad de esta versión del problema (en la que todos los números se representan en unario), son iguales. Por lo tanto, para la función \textbf{f(i, j)} que representa la complejidad de nuestra solución:
\begin{equation*}
    f(i,j) \in O(tam(i)*j)
\end{equation*}
Lo único que cambia en este caso, es que el tamaño de \textbf{i} cuando se mide en términos de su codificación binaria, no resulta en una complejodad exponencial, puesto que la codificación unaria es una menos compacta que la decimal. Debido a esto:
\begin{equation*}
    O(i*j)=O(tam(i), j) \rightarrow f(i, j) \in O(i, j)
\end{equation*}
Bajo mi punto de vista esto no es más que una restructuración de la forma en que se dan los datos (que en lo práctico no resuelve el problema), puesto que al final el número de columnas y filas que se generan para la misma entrada es igual que en el problema original. Sin embargo, desde el lado teórico de la cuestión en este caso, estamos forzando a que la entrada sea de partida representada con una codificación ``binaria/unaria''. Esto resulta en un algoritmo que decide sobre el problema en \textbf{poli-t}, por lo que:
\begin{equation*}
    \text{\textit{UNARY-SSUM}} \in P
\end{equation*}

\end{document}